\section{Scrambling Algorithms}
\label{sec:scrambling_algorithms}

	Due to radiation levels inside the detector chamber, the main data processing takes place in a concrete bunker away from the detector, minimising radiation damage to the hardware.
	To facilitate this, optical linkes, 20 per modual, are used to transfer the data from the front end VELO to the Data Aquizition FPGA (DAQ).
	When comunicating data digitaly, the tranfering modual (TX) and the recieving modual (RX) must have syncrinised clocks.
	When achieving this, there are three main approunches:

	\begin{easylist}
		\ListProperties(Numbers=R,Margin=0.5cm,Align=fixed,FinalSpace=2em)
		& Syncinize both the TX and RX from a single central clock.
		& Transmit the TX clock to the RX modual.
		& Use bit-changes in the data to coninuesly sycnronise the RX clock.
	\end{easylist}

	The two former of these options, although the most convienient, are not appropriate for the VELO as they are suseptable to unforseens delays that could cause desyncronisation.
	The latter, while less suseptable to delays, requires data with a high density of tranitions to reduce the likelyhood of a desyncronisation event.
	Becuase delays in the data are possible, the latter option has been selected.

	\subsection{The Role of Scrambling Data in the VELO}
		
		For the reasons described in Section \ref{sec:scrambling_algorithms}, it is nessesary to ensure that the data has large density of transitions before being transmitted from the front-end detector to the DAQ modual.
		However, as the majority of SP hitmaps are empty, the data has a large bais towards 0s.
		This reduces the frequancy of transitions in the data - increasing the probability of a desyncronisation event.
		It is therefor nesseccary to scramble the data pria to transmition and descramble the data in the DAQ FPGA.
		\par
		Scrambleing and later descrambleing the data is not a trivial exercise.
		The scrambleing (TX) modual and descrambling (RX) modual must use a sycronised \textit{`key'}, derived from the previous states of the data.
		There are two options when generating this \textit{`key'}:

		\begin{description}
			\item[Additive] The \textit{`key'} is generated by evolving the previous \textit{`key'} at each itteration of data using the incoming frame.
			\item[Multiplicative] The  \textit{`key'} is generated from the previos $n$ frames. (Here $n$ is a variable specific to the algorithm).
		\end{description}

	\subsection{Scrambler Options}
	\label{sub:scrambler_options}

		Three scrambling algorithums have been concidered:

		\begin{description}
			\item[Additive Scrambler] \hfill \\
				This algoritum is simple and easy to use, however has the drawback of time dependance.
				If a desyncronisation event occours, all subsequent data is rendered unrecoverable untill such time as a global reset signal is sent.
				Further adding to the drawbacks, if a data packet is not sent from the TX during any clock cycle the RX descrambler will still evolve its descramble key - the TX sccrambler, however, will not.
				This will ofcourse desyncronise the \textit{`keys'}, and as before all subsequent data is lost.

			\item[Intermediate Scrambler] \hfill \\
				Deriving its name from being the second algorithum under concideration, the Intermediate Scrambler is a \textbf{multiplicative} algorithm. 
				The \textit{`key'} is generated from the current incoming frame and the previous frame.
				Therefor, in the event of desyncronisation, only two frames are lost before the \textit{`key'} is automatically recovered.
				This is a significant improvment over the Additive Scrambler.

			\item[VeloPix Scrambler] \hfill \\
				Named as, at the time of the start of the project \textit{(September 2015)}, this algorithum is the current preffered option by the VeloPix team; 
				this too is a \textbf{multiplicative} algorithm.
				The \textit{`key'} is, again like the Intermediate Scrambler, generaged from the currect and previous data frame.
				The VeloPix Scrambler differse from the Intermediate scrambler as it aims to more effeciently scramble the data.
		\end{description}

	\subsection{Algorithm Analysis}

		Intuitively, one can assume that fully scrambled data will be indistinguisable from randomly generated data. 
		For this reason, the three algorithm are not only tested against eachother and the pre-scrambled data but also randomly generated binary.
		The randomly generated data was created using the Python \textit{`random'} library, selecting a \textit{`0'} or \textit{`1'} with probubility $1/2$ each.
		While the Python \textit{`random'} library is only sudo-random, on the scale of this example (i.e. $>$ 100,000 frames), this is by far sufficient.
		\par
		More mathematicaly rigorus, however, is to evaluade the system abstractly in the framework of statistical physics.
		In this abstraction, the ensemble is the 120 bit frame (with the header and parity removed); 
		microstates are the particular form of the frames;
		and macroscopic quantities can be calculated by averaging a large number of frames (i.e. the desync data).
		For the analysis outlined in section \ref{subsub:messurements_of_the_algorithms}, predictions will be made using these principles and outlined in section \ref{subsub:statistical_predictions}.
		\par
		In the context of the statisical model, it is reasonable to concider the degree of \textit{`scrambled-ness'} as entropy.
		Therefor a scrambled system can be assumed to one of maximum entropy; and from Boltzmans law,

		\begin{equation}
			S \sim ln(\Omega)
			\label{eqn:boltzman}
		\end{equation}

		where $\Omega$ is the number of microstates assosiated with the macrostate, we learn that this state of maximum entropy is a macrostate with the maximum number of assosiated microstates.

		\subsubsection{Messurements of the Algorithms} 
		\label{subsub:messurements_of_the_algorithms}

			To compare the effecincy of the three algorithums in section \ref{sub:scrambler_options}, the algorithums where run over the same unput data and compared for the following measures:

			\begin{description}
				\item[Number of Transitions Per Frame] \hfill \\
					This meassure counts the total number of bit transitions (i.e. $bit(n) \neq bit(n-1)$) in a 120 bit frame. 
					The header and parity information was not included as they are not scrambled.
					This is an important test as one of the roles of the scrambler is to maximise the number of transitions.

				\item[Common Bit Chain Length] \hfill \\
					One of the downfalls of the `Number of Transitions Per Frame' analysis is that the two 20 bit frames,

					\begin{enumerate}[a)]
						\item 10101010101111111111
						\item 10011001100110011001
					\end{enumerate}

					both with 10 transitions, will be concidered equal. However, (b) is clearly a more suitable output for data transfer as (a) has a large probability of desyncronisated due to the long chains of \textit{`1'}s.
					It is therefore also nessecary to evaluate the length of common bit chains within the scrambled data. 

				\item[Total Bit Frequancy] \hfill \\
					Pre-scramble, the data had a large bais towards \textit{`0'}s due to the majority of the hitmaps being empty.
					Scrambled data, via entropic arguments, \textit{should} show zero bias eitherway.
					Therefor, by investigating how the number of \textit{`1'}s - \textit{`0'}s evolves over many frames, any bias in the scrambler can be found.

			\end{description}	

		\subsubsection{Statistical Predictions} % (fold)
		\label{subsub:statistical_predictions}

			\begin{description}
				\item[Number of Transitions Per Frame] \hfill \\
					
					Concider a particle in a symmetric, descrete time-dependent, two state system,

					\begin{equation}
						p_0(t) = p_1(t) = 0.5 \quad : \quad \forall\ t \in \mathbb{N}
					\end{equation}

					At each time itteration,

					\begin{equation}
						p_{i \to j}(t) = p_{i \to i}(t) = 0.5 \quad : \quad i,j = [0\ 1], \quad \forall\ t \in \mathbb{N}
					\end{equation}

					However, as $p_{1 \to 0}(t)$ is equal in both probalility and importance to $p_{0 \to 1}(t)$, the probability of a bit change shall herefore be refered to as $p_{t}(t)$.
					\par
					Over a $n$ step process, analogous to a $n$ bit frame, the probalility distribution of the number of transitions $N_t$ is given by Binomial statistics,

					\begin{equation}
						f(N_{t}) = \frac{n!}{N_{t}!(n-N_{t})!}\ p^{N_{t}}\ (1 - p)^{n-N_{t}}
					\end{equation}

					Simplified for the special case $p = p_{t} = 0.5$,

					\begin{equation}
						f_{t}(N_{t}) = \frac{n!}{N_{t}!(n-N_{t})!}\ (p_{t})^{n}
						\label{eqn:transition_propability_dencity}
					\end{equation}

					For $n = 120$, we can calulate,

					\begin{equation}
						<N_t>^{Binomial} \ = \sum_{N_{t}=0}^{n-1} N_{t}\ f(N_{t}) = n\ p_{t} = 60
						\label{eqn:tansition_expectation}
					\end{equation}

					\begin{equation}
						\sigma_{N_t}^{Binomial} = \sqrt{ n\ p_{t}^2} = 5.48
					\end{equation}

					Furthermore, when concidering the entropic argument in section \ref{sub:scrambler_options} equation \ref{eqn:boltzman}, the number of microstates corespoding to each macrostate $N_t$ can be related to equation \ref{eqn:transition_propability_dencity},

					\begin{equation}
						\Omega_t \sim \frac{n!}{N_{t}!(n-N_{t})!}
					\end{equation}

					\begin{equation}
						<N_t>^{Entropic} = MAX[S_t] = MAX[\Omega_t]
					\end{equation}

					This can be numerically solved,

					\begin{equation}
						<N_t>^{Entropic}\ = 60
					\end{equation}
				\item[Common Bit Chain Length] \hfill \\
					
					The probability of a chain of legth $n$ is,

					\begin{equation}
						p_n = (1 - p_t)^n
					\end{equation}

					As $p_t = 0.5$,

					\begin{equation}
						p_n = (p_t)^n
					\end{equation}

					Therefore, the proportion of chains of length 

					\begin{equation}
						add some eqn here \quad what about this?
					\end{equation}

				\item[Total Bit Frequancy] \hfill \\
					
			\end{description}	

		\subsubsection{Results of Analysis}

	\subsection{Conclusion}