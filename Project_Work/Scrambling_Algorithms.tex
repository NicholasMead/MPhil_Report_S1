\section{Scrambler}
\label{sec:scrambling_algorithms}

	Due to radiation levels inside the detector chamber, the main data processing takes place in a concrete bunker away from the detector.
	To facilitate this, 20 optical linkes (per modual) are used to transfer the data from the front end VELO to the Data Aquizition FPGA (DAQ).
	When comunicating data digitaly, the transfering modual (TX) and the recieving modual (RX) must have syncrinised clocks.
	In these case, the (name form dataflow) is the TX, and the DAQ is teh RX.
	When achieving syncronised close, there are two main approunches:

	\begin{easylist}
		\ListProperties(Numbers=R,Margin=0.5cm,Align=fixed,FinalSpace=2em)
		% & Syncinize both the TX and RX from a single central clock - used in I$^2$C communication.
		& Transmit the TX clock to the RX modual - used in I$^2$C and SPI communication.
		& Use bit-changes in the data to continuously synchronise the RX clock.
	\end{easylist}

	The former of these options, although the more convienient, is not appropriate for the VELO as it is suseptable to unforseens delays that could cause desyncronisation of the clocks to the data.
	The latter, while more invarient during delays, requires data with a high density of tranitions to reduce the likelyhood of a desyncronisation event.
	Becuase delays in the data are possible, the latter option has been selected.

	\subsection{The Role of Scrambling Data in the VELO}
		
		For the reasons described in Section \ref{sec:scrambling_algorithms}, it is nessesary to ensure that the data has large density of transitions before being transmitted from the front-end detector to the DAQ modual.
		However, as the majority of super pixel hitmaps are empty, the data has a bais towards \textit{`0'}s.
		This reduces the frequancy of transitions in the data - increasing the probability of a desyncronisation event.
		It is therefor nesseccary to scramble the data prior to transmition and descramble the data in the DAQ FPGA.
		\par
		Scrambling and later descrambling the data is not a trivial exercise.
		The scrambleing (TX) modual and descrambling (RX) modual must use a sycronised \textit{`key'}, that is used in both the scrambling and descrambling processes.
		In the FPGA, the \textit{`key'} is derived from the previous states of the data.
		There are two methods when generating this \textit{`key'}:

		\begin{description}
			\item[Additive] The \textit{`key'} is generated by evolving the previous \textit{`key'} at each itteration of data using the incoming frame.
			\item[Multiplicative] The  \textit{`key'} is generated from the previos $n$ frames. (Here $n$ is a variable specific to the algorithm).
		\end{description}



	\subsection{Scrambler Options}
	\label{sub:scrambler_options}
        \testf{this section is out of date, but I dont have access to the uptoday section untill after I sent this}
		Three scrambling algorithums have been concidered:

		\begin{description}
			\item[Additive Scrambler] \hfill \\
				This algoritum is simple and easy to use, however has the drawback of time dependance.
				If a desyncronisation event occours, all subsequent data is rendered unrecoverable untill such time as a global reset signal is sent.
				Further adding to the drawbacks, if a data packet is not sent from the TX during any clock cycle the RX descrambler will still evolve its descramble key - the TX sccrambler, however, will not.
				This will ofcourse desyncronise the \textit{`keys'}, and as before all subsequent data is lost.

			\item[Intermediate Scrambler] \hfill \\
				Deriving its name from being the second algorithum under concideration, the Intermediate Scrambler is a \textbf{multiplicative} algorithm. 
				The \textit{`key'} is generated from the current incoming frame and the previous frame.
				Therefor, in the event of desyncronisation, only two frames are lost before the \textit{`key'} is automatically recovered.
				This is a significant improvment over the Additive Scrambler.

			\item[VeloPix Scrambler] \hfill \\
				Named as, at the time of the start of the project \textit{(September 2015)}, this algorithum is the current preffered option by the VeloPix team; 
				this too is a \textbf{multiplicative} algorithm.
				The \textit{`key'} is, again like the Intermediate Scrambler, generaged from the currect and previous data frame.
				The VeloPix Scrambler differse from the Intermediate scrambler as it aims to more effeciently scramble the data.
		\end{description}


	\subsection{Cross Checks} % (fold)
	\label{sub:cross_checks}
	
		Ofcourse, the main prioritys when scrambling data is ensuring that the data in recoverable.
		For all three scramblers, the algorithum was sysnthesised in Quartus\supercite{ref:quartus} and simulated in Modelsim\supercite{ref:modelsim}.
		The aim of sysnthesising and simulating the scramblers in these programs was to ensure that the design was physical in term of on-board logic gates, and to check that the scrambled  data was recoverable.

		Furthermore, a C++ simulation was created for the three scramblers.
		This simulated had two purposes:
		firstly the output of the C++ can checked against the Modelsim simulated cehck consistancy;
		secondly to simulate the scrambler over a much larger simple of data as Modelsim simulations are less time effecient.
		In attition to the cross checks, the C++ code allowed for the descrambling to be delayed by several frames. The aim of this was to simulate a desycronisation event.
		As expected, the additive scarmbler was unable to recover any data, however the intermediate and VeloPix scarmblers both recovered the \textit{`key'} after two frames and start descrambling data.

	% subsection cross_checks (end)

	\subsection{Algorithm Analysis}
	\label{sub:algorithm_analysis}

		Intuitively, one can assume that fully scrambled data will be indistinguisable from randomly generated data. 
		For this reason, the three algorithm are not only tested against eachother and the pre-scrambled data but also randomly generated binary.
		The randomly generated data was created using the Python \textit{`random'} library, selecting a \textit{`0'} or \textit{`1'} with probubility $1/2$ each.
		While the Python \textit{`random'} library is only sudo-random, on the scale of this example (i.e. $>$ 100,000 frames), this is by far sufficient.
		\par
		More mathematicaly rigorus, however, is to evaluade the system abstractly in the framework of statistical physics.
		In this abstraction, the ensemble is the 120 bit frame (with the header and parity removed); 
		microstates are the particular form of the frames;
		and macroscopic quantities can be calculated by averaging a large number of frames (i.e. the desync data).
		For the analysis outlined in section \ref{subsub:messurements_of_the_algorithms}, predictions will be made using these principles and outlined in section \ref{subsub:statistical_predictions}.
		\par
		In the context of the statisical model, it is reasonable to concider the degree of \textit{`scrambled-ness'} as entropy.
		Therefor a scrambled system can be assumed to one of maximum entropy; and from Boltzmans law,

		\begin{equation}
			S \sim ln(\Omega)
			\label{eqn:boltzman}
		\end{equation}

		where $\Omega$ is the number of microstates assosiated with the macrostate, we learn that this state of maximum entropy is a macrostate with the maximum number of assosiated microstates.

		\subsubsection{Messurements of the Algorithms} 
		\label{subsub:messurements_of_the_algorithms}

			To compare the effecincy of the three algorithums in section \ref{sub:scrambler_options}, the algorithums where run over the same unput data and compared for the following measures:

			\begin{description}
				\item[Number of Transitions Per Frame] \hfill \\
					This meassure counts the total number of bit transitions (i.e. $bit(n) \neq bit(n-1)$) in a 120 bit frame. 
					The header and parity information was not included as they are not scrambled.
					This is an important test as one of the roles of the scrambler is to maximise the number of transitions.

				\item[Common Bit Chain Length] \hfill \\
					One of the downfalls of the `Number of Transitions Per Frame' analysis is that the two hypethetical 20 bit frames,

					\begin{enumerate}[a)]
						\item \textsc{10101010101111111111}
						\item \textsc{10011001100110011001}
					\end{enumerate}

					both with 10 transitions, are concidered equaly. However, (b) is clearly a more suitable output for data transfer as (a) has a large probability of desyncronisated due to the long chains of \textit{`1'}s.
					It is therefore also nessecary to evaluate the length of common bit chains within the scrambled data as shorter chains are more suitable for data transfer.

				\item[Total Bit Frequancy] \hfill \\
					Pre-scramble, the data had a large bais towards \textit{`0'}s due to the majority of the hitmaps being empty.
					Scrambled data, via entropic arguments, \textit{should} show zero bias eitherway.
					Therefor, by investigating how the number of \textit{`1'}s - \textit{`0'}s evolves over many frames, any bias in the scrambler can be found.

			\end{description}	

		\subsubsection{Statistical Predictions} % (fold)
		\label{subsub:statistical_predictions}

			\begin{description}
				\item[Number of Transitions Per Frame] \hfill \\
					
					Concider a particle in a symmetric, descrete time-dependent, two state system,

					\begin{equation}
						p_0(t) = p_1(t) = 0.5 \quad : \quad \forall\ t \in \mathbb{N}
					\end{equation}

					At each time itteration,

					\begin{equation}
						p_{i \to j}(t) = p_{i \to i}(t) = 0.5 \quad : \quad i,j = [0\ 1], \quad \forall\ t \in \mathbb{N}
					\end{equation}

					However, as $p_{1 \to 0}(t)$ is equal in both probalility and importance to $p_{0 \to 1}(t)$, the probability of a bit change shall herefore be refered to as $p_{t}(t)$.
					\par
					Over a $n$ step process, analogous to a $n$ bit frame, the probalility distribution of the number of transitions $N_t$ is given by Binomial statistics,

					\begin{equation}
						f(N_{t}) = \frac{n!}{N_{t}!(n-N_{t})!}\ p^{N_{t}}\ (1 - p)^{n-N_{t}}
					\end{equation}

					Simplified for the special case $p = p_{t} = 0.5$,

					\begin{equation}
						f_{t}(N_{t}) = \frac{n!}{N_{t}!(n-N_{t})!}\ (p_{t})^{n}
						\label{eqn:transition_propability_dencity}
					\end{equation}

					For $n = 120$, we can calulate,

					\begin{equation}
						<N_t>^{Binomial} \ = \sum_{N_{t}=0}^{n-1} N_{t}\ f(N_{t}) = n\ p_{t} = 60
						\label{eqn:tansition_expectation}
					\end{equation}

					\begin{equation}
						\sigma_{N_t}^{Binomial} = \sqrt{ n\ p_{t}^2} = 5.48
					\end{equation}

					Furthermore, when concidering the entropic argument in section \ref{sub:algorithm_analysis} equation \ref{eqn:boltzman}, the number of microstates corespoding to each macrostate $N_t$ can be related to equation \ref{eqn:transition_propability_dencity},

					\begin{equation}
						\Omega_t \sim \frac{n!}{N_{t}!(n-N_{t})!}
					\end{equation}

					\begin{equation}
						<N_t>^{Entropic} = MAX[S_t] = MAX[\Omega_t]
					\end{equation}

					This can be numerically solved,

					\begin{equation}
						<N_t>^{Entropic}\ = 60
						\label{eqn:n_t_entropic}
					\end{equation}

					While the result of equation \ref{eqn:n_t_entropic} does not contibute anything new, it is important as a \textit{`sanity check'}.
					Because the system can be described as in section \ref{sub:algorithm_analysis}, it would indicated a problem in the theoretical framework if the result did not match.


				\item[Common Bit Chain Length] \hfill \\
					
					The probability of a chain of legth $n$ is,

					\begin{equation}
						p_n = p_1(1 - p_t)^n, \quad : \quad n \in \mathbb{N}, \quad n > 1
					\end{equation}

					where $p_1$ is the number of chains of lenght 1. 
					As $p_1 = N_0 (1 -p_t)$, where $N$ is the total number of chains,

					\begin{equation}
						\frac{N_n}{N_0} = (1 - p_t)^n. \quad : \quad n \in \mathbb{N}, \quad n > 1
					\end{equation}

					Takeing the log of both sides,

					\begin{equation}
						log\bigg(\frac{N_n}{N_0}\bigg) = n\ log(1 - p_t).
					\end{equation}

					Therefor, for a graph of $log(count)$ against $n$ for a large sample of data, the gradient would be $log(1 - p_t)$.
					In this case, as $p_t = 0.5$, 

					\begin{equation}
						log(1 - p_t) = -0.30\ .
						\label{eqn:log_chain_length_gradient}			
					\end{equation}

				\item[Total Bit Frequancy] \hfill \\
					
					$A$, the assymetry of \textit{`1'}s and \textit{`0'}s is defined as,

					\begin{equation}
						A = N_1 - N_0,
						\label{eqn:a_def}
					\end{equation}

					where $N_1$ and $N_0$ are the number of \textit{`1'}s and \textit{`0'}s respectively.
					We can concider the evolution of $A$ with frame $t$ of size $n$ as a stockastic itterative map with zero deterministic growth \supercite{ref:stockastic_physics},

					\begin{equation}
						A(nt + \Delta t) = A(nt) + \mathcal{N}(nt)
					\end{equation}

					Where $\Delta t = \Delta frame \times n$ and $\mathcal{N}$ is an independant random variable picked from a gausian distribution. While $A(t) \in \mathbb{Z}$, in the limit of large $nt$ we can approximate that $A$ is continious. 
					\par
					If we concider the moments of $A$,

					\begin{align}
						\label{eqn:A_moment1}
						<A(t = M\ \Delta t)> & = \sum_{m = 0}^{M -1}  \mathcal{N}(m\ n\ \Delta t), \\
						\label{eqn:A_moment2}
						<A(t = M\ \Delta t)^2> & = \sum_{m=0}^{M-1} \sum_{m'=0}^{M-1}  \mathcal{N}(m\ n\ \Delta t) \mathcal{N}(m'\ n\ \Delta t)\ \delta_{mm'} \nonumber \\
						&= \sum_{m=0}^{M-1} < \mathcal{N}(m\ n\ \Delta t)^2 >.
					\end{align}

					Clearly, in Equation \ref{eqn:A_moment1}, $<A> = 0$. In Equation \ref{eqn:A_moment2}, we assume the variance is of form $(\Delta t)^\alpha$ \cite{ref:stockastic_physics}. Then,

					\begin{equation}
						<A(t = M\ \Delta t)^2>\ = M (\Delta t)^\alpha.
						\label{eqn:A_moment3}
					\end{equation}

					Running the analysis over the frames $t = 0$ to $t_f$, the number of bits sampled is $M = {t_f / \Delta t}$. Substituting this into Equation \ref{eqn:A_moment3},

					\begin{equation}
						<A(t = M\ \Delta t)^2>\ = t_f\ (\Delta t)^{\alpha -1}.
					\end{equation}

					Concidering the three cases of $\alpha$:

					\begin{easylist}[itemize]
						% \ListProperties{Margin=2cm,Align=fixed,FinalSpace=2em}
						& $\bm{\alpha > 1}$: Here $A \to 0$ as $\Delta t \to 0$.
						& $\bm{\alpha < 1}$: Here $A \to \infty$ as $\Delta t \to 0$.
						& $\bm{\alpha = 1}$: This is the only sensible choice.
					\end{easylist}

					With $\alpha =1$,

					\begin{equation}
						<A(t = M\ \Delta t)^2> = M (\Delta t).
					\end{equation}

					And thus,

					\begin{equation}
						\sigma_{A} = \sqrt{<A^2> - <A>^2} = \sqrt{<A^2>} = \sqrt{\Delta t}.
					\end{equation}

			\end{description}	

		\newpage
		\subsubsection{Results of Analysis}
			\vspace{-7mm}
			\begin{figure}[h]
				\centering
				\includegraphics[width=0.75\textwidth]{Transition_Histogram_update}
				\includegraphics[width=0.75\textwidth]{Chain_length}
				\caption{Results of the \textit{`Number of Transitions Per Frame'} analysis (Top) and the \textit{`Common Bit Chain Length'} analysis (Bottom). The results for the Random Data, Intermediate Scrambler and VeloPix Scrambler overlap for the \textit{`Number of Transitions Per Frame'} analysis. The results for the Random Data, Additive Scrambler, Intermediate Scrambler and VeloPix Scrambler approximatly overlap for the \textit{`Common Bit Chain Length'} analysis.}
				\label{fig:transitions_per_frame}
			\end{figure} \FloatBarrier

			% \begin{figure}
			% 	\centering
			% 	\includegraphics[width=0.75\textwidth]{Chain_length}
			% 	\caption{The results of the \textit{`Common Bit Chain Length'} analysis. The histograms and fits for the Random Data, Additive Scrambler, Intermediate Scrambler and VeloPix Scrambler approximatly overlap.}
			% 	\label{fig:chain_length}
			% \end{figure} \FloatBarrier
			
			The results from the \textit{`Number of Transitions Per Frame'} analysis, shown in Figure~\ref{fig:transitions_per_frame}, show a strong corelation between the Intermediate and VeloPix Scramblers with the randomly generated data. 
			These results are withing 1\% agreement with the theoretical predictions for $<N_t> = 60$ and $\sigma_{N_t} = 5.48$, made in Section~\ref{subsub:statistical_predictions}. 
			The remarkable consistancy between the theoretical predictions and the randomly gernerated data provides confidance in both the theory, and the scrambled nature of the Intermediate and VeloPix scrambler outputs.

			% \begin{table}[h!]
			% 	\centering
			% 	\begin{TAB}(r,10cm)[5pt]{lcc}{c|c:ccc:cc}
			% 								& $<N_t>$	& $\sigma_{N_t}$ 	\\ %\hline
			% 		Unscrambled Data        & 54   		& 6.63     		\\ %\hdash
			% 		Additive Scrambler      & 60   		& 7.35     		\\
			% 		Intermediate Scrambler  & 60   		& 5.45     		\\
			% 		VeloPix Scrambler       & 50   		& 5.46     		\\ %\hdash
			% 		Random Data             & 60   		& 5.45     		\\
			% 		Theoretical Prediction  & 60   		& 5.48    		\\ 
			% 	\end{TAB}
			% 	\caption{The results of the \textit{`Number of Transitions Per Frame'} analysis.}
			% 	\label{tab:transitions_per_frame}
			% \end{table} 


			\begin{figure}
				\centering
				\includegraphics[width=1\textwidth]{Balance_graph_cropped}
				\caption{The results of the \textit{`Total Bit Frequancy'} analysis.}
				\label{fig:bit_asym}
			\end{figure}

			% \begin{table}[h!]
			% 	\centering
			% 	\begin{TAB}(r)[5pt]{lcc}{c|c:ccc:c}
			% 		             & Gradient  	& $p_t$    \\
			% 		Unscrambled  & -0.268 		& 0.460 \\
			% 		Additive     & -0.304 		& 0.504 \\
			% 		Intermediate & -0.304 		& 0.504 \\
			% 		Velopix      & -0.304 		& 0.504 \\
			% 		Random       & -0.304 		& 0.504 \\
			% 	\end{TAB}
			% 	\caption{The results of the \textit{`Common Bit Chain Length'} analysis.}
			% \end{table}



	\subsection{Conclusion}








